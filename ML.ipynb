{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5096d3",
   "metadata": {},
   "source": [
    "\n",
    "This notebook is intended to explore various pseudolabelling schemes. Validation, model, data isn't hugely important here, so the cells are collapsed. Keep in mind this is a minimal example without much of the techniques discussed, and this dataset is very basic - adding augmentations, stochastic depth, etc. during training would result in better results. This is intended to be a minimal code example.\n",
    "\n",
    "Reproducibility\n",
    "To make this as fair a comparison as possible, I have seeded random weights and all pseudolabels are produced from the same set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220b53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import  backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb097a",
   "metadata": {},
   "source": [
    "Load Train and Test data and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63991a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x): \n",
    "    return (x-mean_px)/std_px\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "train = pd.read_csv(\"digit-recognizer_train.csv\")\n",
    "test= pd.read_csv(\"digit-recognizer_test.csv\")\n",
    "X_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\n",
    "y_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_test = test.values.astype('float32')\n",
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32) \n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed_everything(seed=42)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
    "\n",
    "# cross validation\n",
    "X = X_train\n",
    "y = y_train\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
    "\n",
    "mnist_test = pd.read_csv(\"mnist_test.csv\")\n",
    "mnist_train = pd.read_csv(\"mnist_train.csv\")\n",
    "ground_truth = pd.read_csv(\"digit-recognizer_sample_submission.csv\")\n",
    "\n",
    "cols = test.columns\n",
    "\n",
    "test['dataset'] = 'test'\n",
    "\n",
    "train['dataset'] = 'train'\n",
    "\n",
    "dataset = pd.concat([train.drop('label', axis=1), test]).reset_index()\n",
    "\n",
    "mnist = pd.concat([mnist_train, mnist_test]).reset_index(drop=True)\n",
    "labels = mnist['label'].values\n",
    "mnist.drop('label', axis=1, inplace=True)\n",
    "mnist.columns = cols\n",
    "\n",
    "idx_mnist = mnist.sort_values(by=list(mnist.columns)).index\n",
    "dataset_from = dataset.sort_values(by=list(mnist.columns))['dataset'].values\n",
    "original_idx = dataset.sort_values(by=list(mnist.columns))['index'].values\n",
    "\n",
    "for i in range(len(idx_mnist)):\n",
    "    if dataset_from[i] == 'test':\n",
    "        ground_truth.loc[original_idx[i], 'Label'] = labels[idx_mnist[i]]\n",
    "        \n",
    "def get_test_acc(model):\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    predictions = np.argmax(predictions,axis=1)\n",
    "\n",
    "    submissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                                \"Label\": predictions})\n",
    "    return accuracy_score(ground_truth['Label'].values, submissions['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc7dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbosity = 0\n",
    "\n",
    "def get_model():\n",
    "    input_1 = tf.keras.layers.Input((28,28,1))\n",
    "    x = tf.keras.layers.Lambda(standardize)(input_1)\n",
    "    x = tf.keras.layers.Convolution2D(32,(3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n",
    "    x = tf.keras.layers.Convolution2D(32,(3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n",
    "    x = tf.keras.layers.Convolution2D(64,(3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n",
    "    x = tf.keras.layers.Convolution2D(64,(3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    out = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=input_1, outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabebdce",
   "metadata": {},
   "source": [
    "No pseudolabelling baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04c3dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pseudolabelling accuracy: 0.99218\n"
     ]
    }
   ],
   "source": [
    "ckp = tf.keras.callbacks.ModelCheckpoint(f'baseline.hdf5', monitor='val_accuracy', verbose=0,\n",
    "                                         save_best_only=True, save_weights_only=True, mode='max')\n",
    "model = get_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=verbosity, callbacks=[ckp])\n",
    "\n",
    "model.load_weights('baseline.hdf5') # load best weights\n",
    "no_pseudo_acc = get_test_acc(model)\n",
    "print(f\"No pseudolabelling accuracy: {format(no_pseudo_acc, '.5g')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00e441",
   "metadata": {},
   "source": [
    "Self training\n",
    "First, we train on the labelled data, then produce pseudolabels and finetune on the pseudolabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98e52fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self training accuracy: 0.99218\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights('baseline.hdf5')\n",
    "\n",
    "pseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\n",
    "pseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\n",
    "pseudolabels = tf.keras.utils.to_categorical(pseudolabels) \n",
    "\n",
    "model.optimizer.lr = 1e-4 # reduce learning rate since we are finetuning\n",
    "\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(f'selftrain.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True,\n",
    "                                         save_weights_only=True, mode='max')\n",
    "model.fit(X_test, pseudolabels, validation_data=(X_val, y_val), epochs=10, batch_size=32,\n",
    "          verbose=verbosity, callbacks=[ckp])\n",
    "\n",
    "model.load_weights('selftrain.hdf5') # load best weights\n",
    "self_train_acc = get_test_acc(model)\n",
    "print(f\"Self training accuracy: {format(self_train_acc, '.5g')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eed62",
   "metadata": {},
   "source": [
    "Simultaneous training\n",
    "First, we train on the labelled data, then initialize a new model and train with labelled data and pseudolabels simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554738ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights('baseline.hdf5')\n",
    "\n",
    "pseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\n",
    "pseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\n",
    "pseudolabels = tf.keras.utils.to_categorical(pseudolabels) \n",
    "y_combined = np.concatenate([pseudolabels, y_train]) # combine our pseudolabels with labelled data\n",
    "X_combined = np.concatenate([X_test, X_train]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67395427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simultaneous training accuracy: 0.99207\n"
     ]
    }
   ],
   "source": [
    "ckp = tf.keras.callbacks.ModelCheckpoint('simultaneous_train.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "model = get_model() # reinitialize model\n",
    "model.fit(X_combined, y_combined, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[ckp], verbose=verbosity) # train a new model on all data together\n",
    "\n",
    "model.load_weights('simultaneous_train.hdf5') # load best weights\n",
    "\n",
    "simultaneous_acc = get_test_acc(model) # get test accuracy\n",
    "print(f\"Simultaneous training accuracy: {format(simultaneous_acc, '.5g')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19abe8",
   "metadata": {},
   "source": [
    "Pretraining\n",
    "First, we train on labelled data, then we create pseudolabels.\n",
    "\n",
    "Next, we initialize a new model and train it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3198551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights('baseline.hdf5')\n",
    "\n",
    "pseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\n",
    "pseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\n",
    "pseudolabels = tf.keras.utils.to_categorical(pseudolabels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f2a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain on pseudolabels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune on labelled data\n",
      "Pretraining accuracy: 0.9923928571428572\n"
     ]
    }
   ],
   "source": [
    "ckp = tf.keras.callbacks.ModelCheckpoint('pretrain.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "print(\"Pretrain on pseudolabels\")\n",
    "model = get_model() # reinitialize model\n",
    "model.fit(X_test, pseudolabels, validation_data=(X_val, y_val), epochs=15, batch_size=32, callbacks=[ckp], verbose=verbosity) # first train on pseudolabels only\n",
    "\n",
    "print(\"Finetune on labelled data\")\n",
    "model.optimizer.lr = 1e-4 # reduce learning rate since we are finetuning\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[ckp], verbose=verbosity) # finetune on labelled data\n",
    "\n",
    "model.load_weights('pretrain.hdf5') # load best weights\n",
    "pretrain_acc = get_test_acc(model) # get test accuracy\n",
    "print(f\"Pretraining accuracy: {pretrain_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a32eb",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "In my talk, I explained the use cases for various pseudolabelling methods. Even though MNIST is not a particularly complex dataset and it's not very fit for pseudolabelling, we still see an improvement over baseline. MNIST's test set is only about half the size of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7914a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pseudolabelling accuracy: 0.9921785714285715\n",
      "Self training accuracy: 0.9921785714285715\n",
      "Simultaneous training accuracy: 0.9920714285714286\n",
      "Pretraining accuracy: 0.9923928571428572\n",
      "------------------------------\n",
      "Percent difference from no pseudolabelling to self training: 0.0%\n",
      "Percent difference from self training to simultaneous training: -0.010798747345308352%\n",
      "Percent difference from simultaneous training to pretraining: 0.03239974080207481%\n",
      "------------------------------\n",
      "Percent difference from no pseudolabelling to pretraining: 0.021597494690616705%\n"
     ]
    }
   ],
   "source": [
    "print(f\"No pseudolabelling accuracy: {no_pseudo_acc}\")\n",
    "print(f\"Self training accuracy: {self_train_acc}\")\n",
    "print(f\"Simultaneous training accuracy: {simultaneous_acc}\")\n",
    "print(f\"Pretraining accuracy: {pretrain_acc}\")\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(f\"Percent difference from no pseudolabelling to self training: {100*(self_train_acc-no_pseudo_acc)/no_pseudo_acc}%\")\n",
    "print(f\"Percent difference from self training to simultaneous training: {100*(simultaneous_acc-self_train_acc)/self_train_acc}%\")\n",
    "print(f\"Percent difference from simultaneous training to pretraining: {100*(pretrain_acc-simultaneous_acc)/simultaneous_acc}%\")\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(f\"Percent difference from no pseudolabelling to pretraining: {100*(pretrain_acc-no_pseudo_acc)/no_pseudo_acc}%\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
